{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os, pickle, cPickle, sys, urllib, gzip, sys\n",
    "sys.setrecursionlimit(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "from IPython.display import Image as IPImage\n",
    "from PIL import Image\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from theano import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import lasagne\n",
    "from lasagne.layers import get_output, InputLayer, DenseLayer, Upscale2DLayer, ReshapeLayer, MergeLayer\n",
    "from lasagne.nonlinearities import rectify, leaky_rectify, tanh,sigmoid\n",
    "from lasagne.updates import nesterov_momentum\n",
    "from lasagne.objectives import categorical_crossentropy\n",
    "from nolearn.lasagne import NeuralNet, BatchIterator, PrintLayerInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda_convnet (faster)\n"
     ]
    }
   ],
   "source": [
    "from lasagne.layers import Conv2DLayer as Conv2DLayerSlow\n",
    "from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerSlow\n",
    "try:\n",
    "    from lasagne.layers.cuda_convnet import Conv2DCCLayer as Conv2DLayerFast\n",
    "    from lasagne.layers.cuda_convnet import MaxPool2DCCLayer as MaxPool2DLayerFast\n",
    "    print 'Using cuda_convnet (faster)'\n",
    "except ImportError, e:\n",
    "    print e\n",
    "    from lasagne.layers import Conv2DLayer as Conv2DLayerFast\n",
    "    from lasagne.layers import MaxPool2DLayer as MaxPool2DLayerFast\n",
    "    print 'Using lasagne.layers (slower)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fname = 'mnist/mnist.pkl.gz'\n",
    "if not os.path.isfile(fname):\n",
    "    testfile = urllib.URLopener()\n",
    "    testfile.retrieve(\"http://deeplearning.net/data/mnist/mnist.pkl.gz\", fname)\n",
    "f = gzip.open(fname, 'rb')\n",
    "train_set, valid_set, test_set = cPickle.load(f)\n",
    "f.close()\n",
    "X, y = train_set\n",
    "X = np.rint(X * 256).astype(np.int).reshape((-1, 1, 28,28)) # convert to (0,255) int range (we'll do our own scaling)\n",
    "mu, sigma = np.mean(X.flatten()), np.std(X.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "eps = 1e-5\n",
    "\n",
    "X_train = X.astype(np.float64) / 255\n",
    "X_train = (1 - 2 * eps) * X_train + eps\n",
    "#X_train =(X_train - mu) / sigma\n",
    "X_train = X_train.astype(np.float32)\n",
    "\n",
    "# we need our target to be 1 dimensional\n",
    "X_out = X_train.reshape((X_train.shape[0], -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9.9999997e-06, 0.99998999)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min(), X_train.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import Layer\n",
    "from lasagne.random import get_rng\n",
    "\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "\n",
    "class Q_Layer(MergeLayer):\n",
    "    \n",
    "    \n",
    "    def __init__(self, incomings, **kwargs):\n",
    "        super(Q_Layer, self).__init__(incomings, **kwargs)\n",
    "        self._srng = RandomStreams(get_rng().randint(1, 2147462579))\n",
    "        \n",
    "    def get_output_shape_for(self, input_shapes):\n",
    "        assert input_shapes[0] == input_shapes[1]\n",
    "        return input_shapes[0]\n",
    "\n",
    "    def get_output_for(self, inputs, deterministic=False, **kwargs):\n",
    "        mu, log_sigma = inputs\n",
    "        \n",
    "        out_shape = mu.shape\n",
    "        \n",
    "        return self._srng.normal(out_shape) * T.exp(log_sigma) + mu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input_var = T.tensor4('inputs')\n",
    "target_var = T.matrix('targets')\n",
    "\n",
    "encode_hid = 256\n",
    "z_hid = 2\n",
    "decode_hid = 256\n",
    "\n",
    "input_layer = InputLayer(shape=(None, X.shape[1], X.shape[2], X.shape[3]), input_var=input_var)\n",
    "reshape_layer = ReshapeLayer(input_layer, shape=(([0], -1)))\n",
    "encode_h_layer = DenseLayer(reshape_layer, num_units=encode_hid, nonlinearity=None)\n",
    "mu_layer = DenseLayer(encode_h_layer, num_units=z_hid, nonlinearity=None)\n",
    "log_sigma_layer = DenseLayer(encode_h_layer, num_units=z_hid, nonlinearity=None)\n",
    "q_layer = Q_Layer([mu_layer, log_sigma_layer])\n",
    "decode_h_layer = DenseLayer(q_layer, num_units=encode_hid, nonlinearity=tanh)\n",
    "network = DenseLayer(decode_h_layer, num_units= X.shape[2] * X.shape[3], nonlinearity=sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Prepare Theano variables for inputs and targets\n",
    "\n",
    "# Create a loss expression for training, i.e., a scalar objective we want\n",
    "# to minimize (for our multi-class problem, it is the cross-entropy loss):\n",
    "prediction = lasagne.layers.get_output(network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def kl_error(mu, log_sigma):  \n",
    "    return 0.5 * T.sum(1 + 2 * log_sigma - T.exp(2 * log_sigma) - T.sqr(mu), axis = 1)\n",
    "\n",
    "x_mu = get_output(mu_layer)\n",
    "x_logs = get_output(log_sigma_layer)\n",
    "\n",
    "kl_loss = kl_error(x_mu, x_logs).mean()\n",
    "\n",
    "rec_loss_raw = lasagne.objectives.binary_crossentropy(prediction, target_var)\n",
    "kl_loss_raw = kl_error(x_mu, x_logs)\n",
    "rec_loss = lasagne.objectives.binary_crossentropy(prediction, target_var).sum(axis = 1).mean()\n",
    "\n",
    "\n",
    "loss = rec_loss - kl_loss\n",
    "#loss = loss.mean()\n",
    "#mse = mse.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "params = lasagne.layers.get_all_params(network, trainable=True)\n",
    "#updates = lasagne.updates.nesterov_momentum(loss, params, learning_rate=0.001, momentum=0.9)\n",
    "updates = lasagne.updates.adam(loss, params)\n",
    "\n",
    "#test_prediction = lasagne.layers.get_output(network, deterministic=True)\n",
    "#test_loss = lasagne.objectives.squared_error(test_prediction, target_var)\n",
    "\n",
    "# Compile a function performing a training step on a mini-batch (by giving\n",
    "# the updates dictionary) and returning the corresponding training loss:\n",
    "train_fn = theano.function([input_var, target_var], [loss, rec_loss, kl_loss, \n",
    "                                                     kl_loss_raw, rec_loss_raw], updates=updates)\n",
    "\n",
    "# Compile a second function computing the validation loss and accuracy:\n",
    "#val_fn = theano.function([input_var, target_var], test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def iterate_minibatches(inputs, targets, batchsize, shuffle=False):\n",
    "    assert len(inputs) == len(targets)\n",
    "    if shuffle:\n",
    "        indices = np.arange(len(inputs))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(inputs) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield inputs[excerpt], targets[excerpt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Epoch 0 of 100 took 1.339s\n",
      "  rec loss:\t\t182.823452\n",
      "  kl loss:\t\t4.274211\n",
      "  training loss:\t187.097663\n",
      "Epoch 50 of 100 took 1.310s\n",
      "  rec loss:\t\t161.175733\n",
      "  kl loss:\t\t5.079783\n",
      "  training loss:\t166.255516\n",
      "Epoch 99 of 100 took 1.309s\n",
      "  rec loss:\t\t160.246496\n",
      "  kl loss:\t\t5.222237\n",
      "  training loss:\t165.468733\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "import time\n",
    "import sys\n",
    "\n",
    "# Finally, launch the training loop.\n",
    "print \"Starting training...\"\n",
    "# We iterate over epochs:\n",
    "for epoch in range(num_epochs):\n",
    "    sys.stdout.flush()\n",
    "    # In each epoch, we do a full pass over the training data:\n",
    "    mse_err = 0\n",
    "    kl_err = 0\n",
    "    loss_err = 0\n",
    "    train_batches = 0\n",
    "    start_time = time.time()\n",
    "    for batch in iterate_minibatches(X_train, X_out, 100, shuffle=True):\n",
    "        inputs, targets = batch\n",
    "        new_loss, new_mse, new_kl, kl_loss_raw, rec_loss_raw = train_fn(inputs, targets)\n",
    "        mse_err += new_mse\n",
    "        kl_err += new_kl\n",
    "        loss_err += new_loss\n",
    "        train_batches += 1\n",
    "    if epoch % 50 == 0 or epoch == num_epochs - 1:\n",
    "        # Then we print the results for this epoch:\n",
    "        print(\"Epoch {} of {} took {:.3f}s\".format(\n",
    "            epoch, num_epochs, time.time() - start_time))\n",
    "        print(\"  rec loss:\\t\\t{:.6f}\".format(mse_err / train_batches))\n",
    "        print(\"  kl loss:\\t\\t{:.6f}\".format(- kl_err / train_batches))\n",
    "        print(\"  training loss:\\t{:.6f}\".format(loss_err / train_batches))\n",
    "        #print kl_loss_raw.shape\n",
    "        #print rec_loss_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 28, 28) (50000, 1, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "output = lasagne.layers.get_output(network)\n",
    "ae_encode = theano.function([input_var], output)\n",
    "X_train_pred = ae_encode(X_train[:1000]).reshape(-1, 28, 28) * 255\n",
    "X_pred = np.rint(X_train_pred).astype(int)\n",
    "X_pred = np.clip(X_pred, a_min = 0, a_max = 255)\n",
    "X_pred = X_pred.astype('uint8')\n",
    "print X_pred.shape , X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "339\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOAAAABwCAAAAAAiICN+AAAFEUlEQVR4nO3cW6zcUxTH8c8451SL\nintdirom7pcgUZcioomQlCDBA57w5EE9iIiIB5FI8CYRQSSIIC4hIS4PBBXiEpdKCEVRt6KKOu0Z\nD7//5MwZM6dnOhLm3/1NJufMnP3f/72y+5u11l7rXwqFQqFQKBQKhf+GxqATnI7LcR7uxjI8ip8G\nnbhiFNtgRyzEKZiLbbEcD+FtrOtx/Rb/0jr+txQDh52+NXgIrsJXOB7HYvuOMQ/jwhncuDnN30ax\nK87F2dgfW2MW/sIvmMC7uB1vYLzLXLXfwWLgsDPaz+Cz8SDmbGTcghnM1Ut/W1TzH45zsAh7ii9s\nin/9vhq7E07FdvK9sFx02TlfrSkGDjsz1uDeEmOOVO9vxk1tfz8Aj2Mf8Y2X4Z4+F9MQrR2Ag0SL\n62UX/sRneAAvi4+8UjR6DBbj4y5z1n4Hi4HDzow1uB6fyL/9b8UXtedg7+NV0eA6rOlzIa34cw5W\nS443LrnfaryGx7Cy+vxL0foJEqMeVV2/oWPe2u9gMXDYmbEGV+JgnIHnNjL2PTzS50Kaop/fq/e/\nYRXexI/42WS+N1KN30G0+1e1vm7UfgeLgcNOX/kgvfV3FS4YbC2aEnOOm9TkKsnxWvljo3otwBHV\n77/iWeVMpp7U3sC+NdiLKzFW/X77Js7RFL1tMKm5bmc3YzhTzm3GxAd2ywXZDHawGDjs/CsaPEnq\nd/A6XtrEeVr1iun0N4Ijcamc36zDE5IzdqP2O1gMHHYG1uDJeEr0MIF78d0A87XrrrOGOCJ1iqVy\nNtQUH3iv5ISddQk2gx0sBg47A2lwS1wt+iM6uGsT5mm0/WxWixqVenwrF2xIzeJ66ZWZkFrhjZIz\nTldvrDXFwGFnRhq8RvTWyX44q+390wMsZKR6zZX+m0WYL7WK8ep1qNQp4RvciSdNzR87qf0OFgOH\nnWk1+DCOwx6Sd70sPmhWl7HLpC6/KYxJPrlAah+Lpc44q1pgQ/TZ2o1vJQe8T2oZvfTHZrCDxcBh\np6sG50j/9SLpB3sRt+EZ3CL5WCfrsLbPmzdEf/NEd2eK9uZKXLu+WstWshMTUjf8SGoRv3XM1aJd\nk7XfwWLgsPMPDe4iPWkLJd+6FTdILLoUV/SYaL7U8D/s4+YN8X8XS0y7I1ZIXX5neTZir+re46K5\nFaL37USb46LNVt2w9IvWjdobOEWDu0k/5kI557gEn0qf2FzxUS2+wOeiu52wr+SD7WM2xlbSa3YR\nZkuO9wF+kDh0X/GTa6RX5heJSefhNDl/fU/i0Zb2igbrRu0NnKLBs6TWAC/I2cht0j/dYkI0skSe\nG1os5yKj4kPHdO9X6cbuUufbRXxbU/zbfBwmz0Stl+ch3hFdzquua/XIHC29oyvke+HH6poWtd/B\nYuCw0/NM5qIeny/FHW3vn8V1kifOll6280Wn09GqNSyQmHND9dnuEnuulR7xT/G8xKEjOLCa+zic\nKP0By/GKxNCd5zO138Fi4LAzRYPfy/MPh3YMekvivmvlPGQ6TpaYdmMabIr2Wmcvo+ID10jPyzLR\n1TJ8XV2ztcSrp0tfafu56R/VZ53UfgeLgcPOFA0+LvWHJR2D7hetzIQX9X6GoZOXxO+dWv3cIOcx\nT4kPXCX+sJXj/Sl6+1zi4NZz9V9X71crzy7Vj9obOPD/JzPozUdMfhF09mx3xpUN2ZFZJn3gbNHp\nWuW5iXpSewMLhUKhUCgUCoVCobB58je0awpqCwb3lAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###  show random inputs / outputs side by side\n",
    "\n",
    "def get_picture_array(X, rescale=4):\n",
    "    array = X.reshape(28,28)\n",
    "    array = np.clip(array, a_min = 0, a_max = 255)\n",
    "    return  array.repeat(rescale, axis = 0).repeat(rescale, axis = 1).astype(np.uint8())\n",
    "\n",
    "def compare_images(index):\n",
    "    print index\n",
    "    original_image = Image.fromarray(get_picture_array(X[index]))\n",
    "    new_size = (original_image.size[0] * 2, original_image.size[1])\n",
    "    new_im = Image.new('L', new_size)\n",
    "    new_im.paste(original_image, (0,0))\n",
    "    rec_image = Image.fromarray(get_picture_array(X_pred[index]))\n",
    "    new_im.paste(rec_image, (original_image.size[0],0))\n",
    "    new_im.save('data/test.png', format=\"PNG\")\n",
    "    return IPImage('data/test.png')\n",
    "\n",
    "#compare_images(2)\n",
    "compare_images(np.random.randint(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ae' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-296-c411fa2429a1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[0mencode_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencode_layer_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_layer_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mae\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'encode'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_output_from_nn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_layer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ae' is not defined"
     ]
    }
   ],
   "source": [
    "## we find the encode layer from our ae, and use it to define an encoding function\n",
    "\n",
    "def get_layer_by_name(net, name):\n",
    "    for i, layer in enumerate(net.get_all_layers()):\n",
    "        if layer.name == name:\n",
    "            return layer, i\n",
    "    return None, None\n",
    "encode_layer, encode_layer_index = get_layer_by_name(ae, 'encode')\n",
    "\n",
    "def get_output_from_nn(last_layer, X, batch_size=128):\n",
    "    indices = np.arange(batch_size, X.shape[0], batch_size)\n",
    "    X_batches = np.split(X, indices)\n",
    "    out = []\n",
    "    for X_batch in X_batches:\n",
    "        out.append(get_output(last_layer, inputs=X_batch).eval())\n",
    "    return np.vstack(out)\n",
    "\n",
    "def encode_input(X):\n",
    "    return get_output_from_nn(encode_layer, X)\n",
    "\n",
    "X_encoded = encode_input(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "next_layer = ae.get_all_layers()[encode_layer_index + 1]\n",
    "final_layer = ae.get_all_layers()[-1]\n",
    "new_layer = InputLayer(shape=(None, encode_layer.num_units))\n",
    "\n",
    "# N.B after we do this, we won't be able to use the original autoencoder , as the layers are broken up\n",
    "next_layer.input_layer = new_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 784)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAHAAAABwCAAAAADji6uXAAABoUlEQVR4nO3av0tWURzH8ZciLRK6\n6FLw0Cgtumm4uYQQRLugLY5u4eLiLtLS4NIfIA6Be9Cg4lIUbYGLOImBTgpSw3mgqevzi4dzvpwv\nXC6Xe859w/sD58e9d+SZ4dbokHkVWIEVWIEVWIEd1NigHziLbw334ystN8NlbGACO9j/T7v4SsvM\ncA6raOEa5w1t4ystM8N1zOARLnDc0Da+0vIyXMECxvEbHx9oH19pWRlO4g0et68/tY+miq+0rAw3\npTH0D06x3UGf+ErLyfAlXkhz4Dm2OuwXX2kZGY7jNZ62rw/wq8O+8ZWWkeFbaS8PX/G+i77xleaf\n4StpHTONG2k/303FV5p3hi1pLz+BW3zGUZfA+ErzznADz6V16Bne9QCMrzTfDGf8W4fe4hB3PQDj\nK803wzVpDoQTfOgRGF9pvhnOt8+XHn6f1lTxleaZ4QqeSGPod3zpAxhfaZ4ZLkrrmCvs9QmMrzS/\nDFvSfn5U+q502icwvtL8MlzCFO7xcwDA+Erzy/CH9E33GrsDAMZXOnTgSP1HuAIrsAIrsO/6C/9N\nMcCVK1gBAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def decode_encoded_input(X):\n",
    "    return get_output_from_nn(final_layer, X)\n",
    "\n",
    "X_decoded = decode_encoded_input(X_encoded[3]) * sigma + mu\n",
    "\n",
    "X_decoded = np.rint(X_decoded).astype(int)\n",
    "X_decoded = np.clip(X_decoded, a_min = 0, a_max = 255)\n",
    "X_decoded  = X_decoded.astype('uint8')\n",
    "print X_decoded.shape\n",
    "\n",
    "pic_array = get_picture_array(X_decoded, np.random.randint(len(X_decoded)))\n",
    "image = Image.fromarray(pic_array)\n",
    "image.save('data/test.png', format=\"PNG\")  \n",
    "IPImage('data/test.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "enc_std = X_encoded.std(axis=0)\n",
    "enc_mean = X_encoded.mean(axis=0)\n",
    "enc_min = X_encoded.min(axis=0)\n",
    "enc_max = X_encoded.max(axis=0)\n",
    "m = X_encoded.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n = 256\n",
    "generated = np.random.normal(0, 1, (n, m)) * enc_std + enc_mean\n",
    "generated = generated.astype(np.float32).clip(enc_min, enc_max)\n",
    "X_decoded = decode_encoded_input(generated) * sigma + mu\n",
    "X_decoded = np.rint(X_decoded ).astype(int)\n",
    "X_decoded = np.clip(X_decoded, a_min = 0, a_max = 255)\n",
    "X_decoded  = X_decoded.astype('uint8')\n",
    "!mkdir -p montage\n",
    "for i in range(n):\n",
    "    pic_array = get_picture_array(X_decoded, i, rescale=1)\n",
    "    image = Image.fromarray(pic_array)\n",
    "    image.save('montage/{0:03d}.png'.format(i), format='png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: montage: not found\r\n"
     ]
    },
    {
     "data": {
      "image/png": "bW9udGFnZS5wbmc=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!montage -mode concatenate -tile 16x montage/*.png montage.png\n",
    "IPImage('montage.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
